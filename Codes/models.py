# -*- coding: utf-8 -*-
"""Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1382OAOW1xjzF2dsmV2Dsj4PB8x8eIXSS

#Requirements
1. Numpy
2. Pandas
3. Scipy
4. matplotlib
5. seaborn
6. sklearn
7. plotly
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from scipy.stats import randint
import pandas as pd # data processing, CSV file I/O, data manipulation 
import matplotlib.pyplot as plt # this is used for the plot the graph 
import seaborn as sns # used for plot interactive graph. 
from pandas import set_option
plt.style.use('ggplot') # nice plots

from sklearn.model_selection import train_test_split # to split the data into two parts
from sklearn.linear_model import LogisticRegression # to apply the Logistic regression
from sklearn.feature_selection import RFE
from sklearn.model_selection import KFold # for cross validation
from sklearn.model_selection import GridSearchCV # for tuning parameter
from sklearn.model_selection import RandomizedSearchCV  # Randomized search on hyper parameters.
from sklearn.preprocessing import StandardScaler # for normalization
from sklearn.pipeline import Pipeline 
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.neighbors import KNeighborsClassifier #KNN
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB

import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv("Credit_default_dataset.csv")

#For easy Understanding changing some names
df.rename(columns={'default.payment.next.month':'defaulter'}, inplace=True)
df.rename(columns={'PAY_0':'PAY_1'}, inplace=True)

fil = (df['EDUCATION'] == 5) | (df['EDUCATION'] == 6) | (df['EDUCATION'] == 0)
df.loc[fil, 'EDUCATION'] = 4
df['EDUCATION'].value_counts()

fil = (df['MARRIAGE'] == 0)
df.loc[fil, 'MARRIAGE'] = 3
df['MARRIAGE'].value_counts()



X = df.drop('defaulter', axis=1)  
y = df['defaulter']

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify=y, random_state=42)

features = df.drop('defaulter', axis = 1, inplace = False)
stdX = (features - features.mean()) / (features.std()) 
Xstd_train, Xstd_test, ystd_train, ystd_test = train_test_split(stdX,y, test_size=0.2, stratify=y,
                                                                random_state=42)

Xstd_train.describe()



def model_eval(algo, Xtrain,ytrain,Xtest,ytest):
    from sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve,classification_report

    algo.fit(Xtrain,ytrain)
    y_pred = algo.predict(Xtrain)
    y_train_prob = algo.predict_proba(Xtrain)[:,1]

    #print('confusion matrix-train\n',confusion_matrix(ytrain,y_pred))
    print('Overall Train Accuracy',accuracy_score(ytrain,y_pred))
    print('Train AUC Score',roc_auc_score(ytrain,y_train_prob))

    y_test_pred = algo.predict(Xtest)
    y_test_prob = algo.predict_proba(Xtest)[:,1]


    #print('confusion matrix-test\n',confusion_matrix(ytest,y_test_pred))
    print('Overall Test Accuracy',accuracy_score(ytest,y_test_pred))
    print('Test AUC Score',roc_auc_score(ytest,y_test_prob))
    print('Classification Report of Test\n',  classification_report(ytest, y_test_pred))
    
    
    kf = KFold(n_splits = 5,shuffle = True,random_state = 42)
    score=[]
    for train_idx,test_idx in kf.split(Xtrain,ytrain):
        xtrain_k,xtest_k = Xtrain.iloc[train_idx,:],Xtrain.iloc[test_idx,:]
        ytrain_k,ytest_k = ytrain.iloc[train_idx],ytrain.iloc[test_idx]
        algo.fit(xtrain_k,ytrain_k)
        y_pred_k = algo.predict(xtest_k)
        roc = roc_auc_score(ytest_k,y_pred_k)
        score.append(roc)
    print('K-Fold scores: %0.03f (+/- %0.5f)' % (np.mean(score),np.var(score,ddof=1)))
    
    f,ax =  plt.subplots(1,2,figsize=(14,6))
    #plt.figure(figsize=(6,4))
    ConfMatrix = confusion_matrix(ytest,y_test_pred)
    sns.heatmap(ConfMatrix,annot=True, cmap='YlGnBu', fmt="d", 
            xticklabels = ['Non-default', 'Default'], 
            yticklabels = ['Non-default', 'Default'],linewidths=.5,ax = ax[0])
    ax[0].set_ylabel('True label')
    ax[0].set_xlabel('Predicted label')
    ax[0].set_title('Confusion Matrix')

    global fpr,tpr,thresholds
    fpr,tpr,thresholds = roc_curve(ytest,y_test_prob)
    ax[1].plot(fpr,tpr,color = 'r')
    ax[1].plot(fpr,fpr,color = 'green')
    ax[1].set_ylabel('TPR')
    ax[1].set_xlabel('FPR')
    ax[1].set_title('ROC Curve')
    plt.show()
    
    return (accuracy_score(y_test, y_test_pred),precision_score(y_test, y_test_pred,average='weighted'),recall_score(y_test, y_test_pred,average='weighted')
    ,f1_score(y_test,y_test_pred,average='weighted'))

"""## Benchmark Model: Logistic Regression"""

from sklearn.linear_model import LogisticRegression

LR = LogisticRegression(random_state=0)

accuracy_LR,precision_LR, recall_LR, f1_score_LR = model_eval(LR,Xstd_train,ystd_train,Xstd_test,ystd_test )

"""# Different Models

### 1. Naive Bayes
"""

clf = GaussianNB()

accuracy_NB,precision_NB, recall_NB, f1_score_NB = model_eval(clf,Xstd_train,ystd_train,Xstd_test,ystd_test )

"""### 2. KNN"""

knn = KNeighborsClassifier(n_neighbors=5)

accuracy_KNN,precision_KNN, recall_KNN, f1_score_KNN = model_eval(knn, Xstd_train,ystd_train,Xstd_test,ystd_test)

"""### 3. Decision Trees"""

DT = DecisionTreeClassifier(criterion= 'entropy', max_depth= 4, 
                                     max_features= 7, min_samples_leaf= 8, 
                                     random_state=0)

accuracy_DT,precision_DT, recall_DT, f1_score_DT = model_eval(DT, Xstd_train,ystd_train,Xstd_test,ystd_test)



"""### 4. Random Forest"""

RF = RandomForestClassifier(criterion= 'gini', max_depth= 6, 
                                     max_features= 5, n_estimators= 150, 
                                     random_state=0)

accuracy_RF,precision_RF, recall_RF, f1_score_RF = model_eval(RF, Xstd_train,ystd_train,Xstd_test,ystd_test)



"""### 5. GradientBoostingClassifier"""

from sklearn.ensemble import GradientBoostingClassifier
GBC = GradientBoostingClassifier(random_state=10)

accuracy_GBC,precision_GBC, recall_GBC, f1_score_GBC = model_eval(GBC, Xstd_train,ystd_train,Xstd_test,ystd_test)



"""### 6. AdaBoostClassifier"""

from sklearn.ensemble import AdaBoostClassifier
ABC = AdaBoostClassifier(random_state=10)

accuracy_ABC,precision_ABC, recall_ABC, f1_score_ABC = model_eval(ABC, Xstd_train,ystd_train,Xstd_test,ystd_test)



"""### 7. VotingClassifier"""

from sklearn.ensemble import VotingClassifier
VC = VotingClassifier(estimators=[('rf',RF),('gb', GBC),('ab',ABC)],voting='soft')

accuracy_VC,precision_VC, recall_VC, f1_score_VC = model_eval(VC, Xstd_train,ystd_train,Xstd_test,ystd_test)



"""# Comparison"""

list_accuracy=[]
list_accuracy.append(accuracy_LR)
list_accuracy.append(accuracy_NB)
list_accuracy.append(accuracy_KNN)
list_accuracy.append(accuracy_DT)
list_accuracy.append(accuracy_RF)
list_accuracy.append(accuracy_GBC)
list_accuracy.append(accuracy_ABC)
list_accuracy.append(accuracy_VC)

list_precision=[]
list_precision.append(precision_LR)
list_precision.append(precision_NB)
list_precision.append(precision_KNN)
list_precision.append(precision_DT)
list_precision.append(precision_RF)
list_precision.append(precision_GBC)
list_precision.append(precision_ABC)
list_precision.append(precision_VC)

list_recall=[]
list_recall.append(recall_LR)
list_recall.append(recall_NB)
list_recall.append(recall_KNN)
list_recall.append(recall_DT)
list_recall.append(recall_RF)
list_recall.append(recall_GBC)
list_recall.append(recall_ABC)
list_recall.append(recall_VC)

list_f1_score=[]
list_f1_score.append(f1_score_LR)
list_f1_score.append(f1_score_NB)
list_f1_score.append(f1_score_KNN)
list_f1_score.append(f1_score_DT)
list_f1_score.append(f1_score_RF)
list_f1_score.append(f1_score_GBC)
list_f1_score.append(f1_score_ABC)
list_f1_score.append(f1_score_VC)

##Comapare Accuracy

import matplotlib.pyplot as plt
classifier_names=('Log_reg','Naive_Bayes','KNN',"Decision Trees",'random_forst','grad_boost','adaboost','voting_class')
y_axis=np.arange(len(classifier_names))
plt.figure(figsize=(12,8))
plt.bar(y_axis, list_accuracy, alpha=0.4,color='Blue',ecolor='navy')
plt.xticks(y_axis, classifier_names)
plt.ylabel('Accuracy Scores')
plt.title('Classifier comparison wrt Accuracy')
plt.show()

import matplotlib.pyplot as plt
classifier_names=('Log_reg','Naive_Bayes','KNN',"Decision Trees",'random_forst','grad_boost','adaboost','voting_class')
y_axis=np.arange(len(classifier_names))
plt.figure(figsize=(12,8))
plt.bar(y_axis, list_precision, alpha=0.4,color='Purple',ecolor='darkgreen')
plt.xticks(y_axis, classifier_names)
plt.ylabel('Accuracy Scores')
plt.title('Classifier comparison wrt Precison')
plt.show()

import matplotlib.pyplot as plt
classifier_names=('Log_reg','Naive_Bayes','KNN',"Decision Trees",'random_forst','grad_boost','adaboost','voting_class')
y_axis=np.arange(len(classifier_names))
plt.figure(figsize=(12,8))
plt.bar(y_axis, list_recall, alpha=0.4,color='orange',ecolor='black')
plt.xticks(y_axis, classifier_names)
plt.ylabel('Accuracy Scores')
plt.title('Classifier comparison wrt Recall')
plt.show()

import matplotlib.pyplot as plt
classifier_names=('Log_reg','Naive_Bayes','KNN',"Decision Trees",'random_forst','grad_boost','adaboost','voting_class')
y_axis=np.arange(len(classifier_names))
plt.figure(figsize=(12,8))
plt.bar(y_axis, list_f1_score, alpha=0.4,color='orange',ecolor='black')
plt.xticks(y_axis, classifier_names)
plt.ylabel('Accuracy Scores')
plt.title('Classifier comparison wrt f1_score')
plt.show()

"""### Neural Networks"""

from keras.initializers import glorot_uniform
import keras
from keras.models import Sequential
from keras.layers import Dense

hl   = 4                  # number of hidden layer
nohl = [50,30,20,10]     # number of neurons in each hidden layer

classifier = Sequential()

# Hidden Layer
for i in range(hl):
    if i==0:
        classifier.add(Dense(units=nohl[i], input_dim=X_train.shape[1], kernel_initializer='uniform', activation='relu'))
    else :
        classifier.add(Dense(units=nohl[i], kernel_initializer=glorot_uniform(seed=0), activation='relu'))

# Output Layer
classifier.add(Dense(units=1, kernel_initializer=glorot_uniform(seed=0), activation='sigmoid'))

classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

classifier.summary()

classifier.fit(Xstd_train, ystd_train, epochs=60, batch_size=512)

y_pred = classifier.predict(Xstd_test)
y_pred = (y_pred > 0.5)
conf_matr = confusion_matrix(ystd_test, y_pred)

sns.heatmap(conf_matr,annot=True, cmap='YlGnBu', fmt="d", 
            xticklabels = ['Non-default', 'Default'], 
            yticklabels = ['Non-default', 'Default'],linewidths=.5)

print('Classification Report of Test\n',  classification_report(ystd_test, y_pred))



"""## Some Tuning of parameter of model Random Forest, Adaboost, GradientBoost

1. Random Forest and GradientBoost
"""

parameters_boost ={'n_estimators' :[15,25,50],
             'min_samples_split' :[3,10,15]
}

GBC_refine = GridSearchCV(GradientBoostingClassifier(random_state=10), cv=10, param_grid=parameters_boost, scoring='accuracy')
GBC_refine.fit(Xstd_train,ystd_train)
print("Best parameters for GradeintBoosting:",GBC_refine.best_params_)


RF_refine = GridSearchCV(RandomForestClassifier(random_state=10), cv=10, param_grid=parameters_boost,scoring='accuracy')
RF_refine.fit(Xstd_train,ystd_train)
print("Best parameters for RandomForest:",RF_refine.best_params_)

"""2. Adaboost"""

parameters_ada ={'n_estimators' :[15,25,50],
            }
ABC_refine = GridSearchCV(AdaBoostClassifier(random_state=10), parameters_ada,cv=10,scoring='accuracy')
ABC_refine.fit(Xstd_train, ystd_train)
print("Best parameters for Adaboost:",ABC_refine.best_params_)

from sklearn.calibration import CalibratedClassifierCV
clfvc1 = RandomForestClassifier(n_estimators=50,min_samples_split=15,random_state=10)
clfvc2=AdaBoostClassifier(n_estimators=50,random_state=10)
clfvc3=GradientBoostingClassifier(n_estimators=25,min_samples_split=3,random_state=10)

clf_isotonic1 = CalibratedClassifierCV(clfvc1, method='isotonic',cv=10)
clf_isotonic2 = CalibratedClassifierCV(clfvc2, method='isotonic',cv=10)
clf_isotonic3 = CalibratedClassifierCV(clfvc3, method='isotonic',cv=10)

VC_refine = VotingClassifier(estimators=[('rf',clf_isotonic1),('ab', clf_isotonic2),('gb',clf_isotonic3)],voting='soft',weights=[1,1,1])
VC_refine.fit(X_train,y_train)

y_pred = VC_refine.predict(Xstd_test)
conf_matr = confusion_matrix(ystd_test, y_pred)
sns.heatmap(conf_matr,annot=True, cmap='YlGnBu', fmt="d", 
            xticklabels = ['Non-default', 'Default'], 
            yticklabels = ['Non-default', 'Default'],linewidths=.5)

print('Classification Report of Test\n',  classification_report(ystd_test, y_pred))



